1. Basic information
Team Number : 11
Student ID# of Submitter: 1375886
Name of Submitter: Travis Takai
ID#s and Names for others on the Team
- Brad Bernard 1426712
- Johannes Pitz 1442286

2. Meta-data
- Show your meta-data design (Tables and Columns table) and information about each column.

We started with the schema presented in the project PDF. The "tables" table initially consisted of: table-id(int), table-name(vc-50), file-name(vc-50). 
However after reading through the full description of the Catalog we realized we need an extra field: privilege(int) which signaled whether it was a system
table or a user table. Our columns table is an exact copy of the one in the PDF: table-id(int), column-name(vc-50), column-type(int), column-length(int),
column-position(int). We named the tables table and the columns table: "Tables" and "Columns" since the test suite rmtest expected those names. 

On create catalog, we create the .tbl files (our filename extension for a table file) for columns and tables. Then we insert the rows corresponding to those
two new tables into themselves so they are self described. Usually inserting into a table we have to scan the tables table by table-name
and then scan the columns table with the retuned table-id. But since the columns table and the tables table was static, we created a method that holds the 
static vector of Attributes for both of the tables (rm::columnsColumns() and rm::tablesColumns()). After we had the record descriptors, we also created helper
methods to insert a record in each of the system tables (rm::insertTableRecord() and rm::insertColumnRecord()). These helper functions called another helper 
function to prepare the data in the desired record format (rm::prepareTableRecord() and rm::prepareColumnRecord()). After the helper functions were created,
it made it easy to insert these predefined rows into tables and columns so we created a function to insert these static rows into each of the corresponding
tables (rm::insertTableRecords() and rm::insertColumnRecords()). These functions just called the non plural version  (rm::insertTableRecord() and rm::insertColumnRecord()) 
multiple times for each row (2 rows into tables, 9 records into columns).

After we created these sets of helper functions for both of the system tables, we realized we needed a way to track the auto increment id on the tables table. 
We decided to create a new file "tables.stat" that holds a single integer of the current maximum table id in the tables table. So in RM constructor, we setup
a file handle to access this statistic file for reading and writing. If it doesn't exist then create it else use the existing file. Whenever we want to create
a new table (rm::insertTableRecord()) we call our helper function rm::getMaxTableId() to get the current max Id and then insert the new row with one higher than
that value. After we insert our record we write our new max Id to our stat file through rm::setMaxTableId(). 

3. Iterator
- Describe design and implementation of the RBFM iterator/scan and the RM iterator/scan and their associated methods.

RM iterator/scan:
We decided that the RBFM iterator would hold all the information passed to the rm::scan() method as private variables. So rm::scan()
created took in an RM iterator and setup the iterator by calling the rbfm::scan() method with the correct values. However, before we passed it
to the rbfm::scan() we made sure the projection and condition attribute columns existed in the table they were querying on by scanning the columns/tables
tables and cross referencing the passed in columns through an unordered_set difference. We also had to pass in a valid column for condition attribute
so on NO_OP conditional operator we passed in the first column of the returned schema from our columns scan. After all the data checked out, we finally
passed the target scan data into the child rbfm::scan() method which saved all the arguments and setup the scan. 
NextTuple then calls NextRecord.

RBFM iterator/scan:
In the scan function we save all the arguments that were passed in, and collect all the indexes of the columns that we want to return. 
We also save the index of conditionAttribute, a currPage and currSlot.
In the nextRecord function we have a loop that reads a new page if necessary (if this fails if we reach the last page, so we return EOF on a readPage() error).
Then we look at the next record, first we check if it satisfies the condtion, and then we copy the wanted attributes into *data using the correct recordDescriptor
plus data format.

4. Internal Record Format
- Show your record format design and describe how your design satisfies O(1) field access. If not, just mention that your team hasn't implemented this feature.
Our records use the following structure:
| Field count | Null byte(s) | Directory of offsets to end of field value | Values for Fields |
We based the structure off of the variable length record-format on slide 11 of lecture 3. The field count and all of our directory offsets use a 
uint16_t since it is large enough to represent any feasible offset and memory efficient since it is half the bytes of an int and unsigned! It provides O(1) 
accessibility because we know how many fields are present and/or null and we can calculate the offset directly to a field.
We have an directory entry even for fields that are null, but there is no space wasted for the data since the difference between the current offset
and the previous offset would be zero, and of course we have null for that field in our null bits.

- Describe how you store a VarChar field.
The offset of a field minus the previous offset determines the length, so we can avoid padding the varchar if it is less than the varchar length size.
After we calculate the length of the varchar field, we just grab the bytes, just like any other field, and convert it into a string.
When we receive a record, if the incoming record attribute vector has a varchar type, then we read the first four bytes as an integer to determine the length of the data.

- Describe how you deal with an update and delete.
Delete removes the entire record from the page (and then compacts the other records, more details in Section 5). 
Update removes the entire record and then inserts its (possibly on the same page, more details in Section 5).

5. Page Format
- Show your page format design.
On a page, the last 2 bytes represents the free space pointer (offset that is a uint16_t byte offset within the page).
The second to last 2 bytes on the page represents the number of records on the page (uint16_t count).
After that, there are 4 byte entries corresponding to our directory for finding records on a page.
The directory entries are ordered by slot number increasing and contain two bytes for the offset of a record within page (int16_t) and two bytes 
for the length of record (uint16_t)). Our page has records at the top, free space in the middle (pointed to by free space pointer), and our directory at the bottom.
The directory grows from bottom up and the records grow from top to bottom so we make sure there is enough space so we don't overwrite anything.
The rid slot num's begin with 1 and not with 0 (could easily be fixed, but there were no requirements on this).

- Describe how you deal with an update and delete.
Delete uses the RID to find offset and length of the record and then memmove(2) (not memcpy because of overlapping blocks) everything between offset and freeSpace.
Then we walk through the directory and update all the offsets that were higher than the offset of the deleted record. Also we overwirte the directory entry of the 
deleted records offset with (const int16_t deleted_entry = 0x8000;)
Update calls delete and then tries to insert the new record into on the same page. If this succeds then we update directory entry. If it fails then we just call
insertRecord and upate the directory entry on the original page to point to the new RID.
To foreward to another RID we save -slotNum in the offset (which is a int16_t), and the pageNum in the length space (uint16_t). This allows 
us to foreward to up to page 2^16 - 1 (therefore we are limited to a max file size of 2^16 * 4K = 256MB).

6. File Format
- Show your file format design
Our file format is a .tbl file for each table that is created. All tables created are case sensitive names and filenames. Our catalog tables are 
capitalized first letter: Tables.tbl and Columns.tbl. The actual format for the .tbl file is just a malloc(PAGE_SIZE = 4096) stored in a binary file.
We get the number of pages in our file by calling stat() on the file and getting the total size in bytes then diving by PAGE_SIZE. Everytime we read
or write we fseek to a desired page: PAGE_SIZE * pageNum and after our writes we call fflush(file*) to ensure our writes are persistent to the hdd.

7. Implementation Detail
- Other implementation details goes here.
Created an external file table.stat that tracks the tables table current inserted maximum ID. We make sure to delete the file on deleting the 
catalog and create it when we create the catalog. This is documented above in meta data design last paragraph.

8. Other (optional)
In codebase/rm there is a file _rmtest.cc with in which we got rid of all the memory leak that was in the testscript (except the nullindicators in each test)
Note: to be able to have no leaks you need to call a function to destroy the instance. Ours is called rm->DestroyInstance(), and this function actually destroys 
all the underlying instances.